{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"Copy of openmp_hello_world.ipynb","provenance":[{"file_id":"https://github.com/apanqasem/notebooks/blob/main/openmp/openmp_hello_world.ipynb","timestamp":1619456152544}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"718b9f37"},"source":["# Hello World in OpenMP\n","\n","### Description\n","\n","A very basic introduction to OpenMP. The demo walks through a Hello World program parallelized with\n","the `omp parallel` directive and discusses the importance and significance of thread count\n","using a matrix-scalar multiplication example. \n","\n","Covers the following directives and API\n","\n","  * OpenMP directives: `parallel` and `parallel for`\n","  * OpenMP API: `omp_set_num_threads()`, `omp_get_thread_num()`\n","  \n","### Outline\n","\n","   * [Installing OpenMP](#install)\n","   * [Compiling and Running an OpenMP Program](#compile)\n","   * [OpenMP Compiler Directives](#directives)\n","   * [OpenMP Runtime API](#api)\n","   * [Performance Evaluation](#timing)\n","   * [Thread Count ans Scalability](#thread_count)\n","   \n","### <a name=\"install\"></a> Installing OpenMP\n","\n","OpenMP does not need to be installed separately. It is packaged with the compiler on your\n","system. Check the GCC version to make sure the compiler supports OpenMP"],"id":"718b9f37"},{"cell_type":"code","metadata":{"id":"14628192"},"source":["!gcc --version"],"id":"14628192","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"342414d7"},"source":["OpenMP has been supported since GCC 4.2, which implements OpenMP specification 2.5. To\n","ensure support for newer OpenMP specifications we need to have a recent version of GCC\n","installed. In particular, we want at least GCC 6 which provides support for OpenMP 4.5\n","which added significant enhancements over earlier versions. \n","\n","### <a name=\"compile\"></a>Compiling and Running an OpenMP Program\n","\n","To compile an OpenMP program, all that is needed is to pass the appropriate compiler flag. For GCC\n","(and Clang) this flag is `-fopenmp`. Consider the following Hello World C program."],"id":"342414d7"},{"cell_type":"code","metadata":{"attributes":{"classes":["C"],"id":""},"id":"bbc48bd5"},"source":["#include<stdio.h>\n","\n","int main() {\n","\n","  printf(\"Hello World\\n\");\n","  printf(\"Goodbye World!\\n\");\n","  return 0;\n","}"],"id":"bbc48bd5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a613a62e"},"source":["The above can be compiled with OpenMP with the following"],"id":"a613a62e"},{"cell_type":"code","metadata":{"id":"81fc1067"},"source":["!gcc -o hello -fopenmp hello.c"],"id":"81fc1067","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6580bebf"},"source":["We can run the resulting executable in the same manner as we would a regular sequential program"],"id":"6580bebf"},{"cell_type":"code","metadata":{"id":"89cd01c3"},"source":["!./hello"],"id":"89cd01c3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bba50f4c"},"source":["Of course, we have not added any parallelism in the code yet. So the result is uninteresting. But\n","even after adding parallelism, the process of compiling and running OpenMP programs will remain the\n","same. \n","\n","\n","### <a name=\"directives\"></a>OpenMP Compiler Directives\n","\n","To parallelize with OpenMP we need to add directives or pragmas in the source code. OpenMP supports\n","a wide [range of\n","pragmas](https://www.openmp.org/wp-content/uploads/OpenMP-4.5-1115-CPP-web.pdf). The most simplest\n","of these is the `parallel` pragma. Let us insert the pragma in our Hello World code."],"id":"bba50f4c"},{"cell_type":"code","metadata":{"attributes":{"classes":["C"],"id":""},"id":"da1e9f71"},"source":["#pragma omp parallel {\t\t\n","  printf(\"Hello World!\\n\");\n","  printf(\"Goodbye World!\\n\");\n","}"],"id":"da1e9f71","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0770ba9f"},"source":["All pragmas in OpenMP begin with `#pragma omp`. This is usually followed by a keyword which\n","describes the action to be performed. The action can be followed by a sequence of _clauses_ to\n","influence the prescribed action. For now, we will just look at the `parallel` pragma without any\n","clauses. A `pragma` is usually followed by a pair curly braces to mark the block of code on which\n","the action is to be performed. With the braces, the pragma will apply to the next statement only,\n","which is a behavior, we rarely want. \n","\n","We can now attempt to compile the OpenMP code using the `fopenmp` flag."],"id":"0770ba9f"},{"cell_type":"code","metadata":{"id":"4586c5d7"},"source":["!gcc -o hello -fopenmp hello0.c "],"id":"4586c5d7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eb19976d"},"source":["_What happened?_ \n","\n","The compiler error message is a little cryptic. The problem here is that the \n","opening `{` must be on a new line. If you prefer the style where the opening brace is placed on the\n","same line as the statement preceding a code block then it may take a little getting used to. The\n","above code can be fixed by simply moving the opening braces to the next line."],"id":"eb19976d"},{"cell_type":"code","metadata":{"attributes":{"classes":["C"],"id":""},"id":"5c61187e"},"source":["#pragma omp parallel \n","{\t\t\n","  printf(\"Hello World!\\n\");\n","  printf(\"Goodbye World!\\n\");\n","}"],"id":"5c61187e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c748f2d5"},"source":["We can now build the code successfully."],"id":"c748f2d5"},{"cell_type":"code","metadata":{"id":"27158ae2"},"source":["!gcc -o hello -fopenmp hello.c"],"id":"27158ae2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fc6f7217"},"source":["_What do we expect the output to be?_\n","\n","Let's run the program"],"id":"fc6f7217"},{"cell_type":"code","metadata":{"id":"2c32b306"},"source":["!./hello "],"id":"2c32b306","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a07dfab2"},"source":["The behavior may not be exactly what you expected. Here's how the `parallel` directive works. \n","\n","  * the pragma marks a _parallel_ region in the program\n","  * at runtime OpenMP creates _n_ threads where _n_ is determined from the environment\n","  * each thread executes each statement in the block in parallel (i.e., an instance of block is\n","    executed _n_ times) \n","\n","_Can we find out how many threads OpenMP created for the Hello World program?_\n","\n","### <a name=\"api\"></a>OpenMP Runtime Library Routines \n","\n","We can use `wc` to count the number of lines in the output."],"id":"a07dfab2"},{"cell_type":"code","metadata":{"id":"b79c4b51"},"source":["!./hello | wc -l"],"id":"b79c4b51","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ec6e963"},"source":["_Why did OpenMP decide to create 12 threads?_ \n","\n","Generally, OpenMP will try to match the number threads to the available processing cores. Let's\n","check the number of available cores in our system"],"id":"8ec6e963"},{"cell_type":"code","metadata":{"id":"4a3c1d6c"},"source":["!lscpu | head -4"],"id":"4a3c1d6c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0a783af"},"source":["We can modify this default behavior in several ways. One way to do this is via a call to [OpenMPs\n","runtime library](https://gcc.gnu.org/onlinedocs/libgomp/Runtime-Library-Routines.html). OpenMP\n","supports a large collection of runtime routines. To use these routines, we need include the OpenMP\n","header file."],"id":"b0a783af"},{"cell_type":"code","metadata":{"id":"227699aa"},"source":["#include<omp.h>"],"id":"227699aa","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f6a735f5"},"source":["We can then tell OpenMP to use a specific number of threads using the appropriately named function\n","`omp_set_num_threads()`"],"id":"f6a735f5"},{"cell_type":"code","metadata":{"id":"a79a6029"},"source":["omp_set_num_threads(4)"],"id":"a79a6029","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c9513b73"},"source":["Each thread created by OpenMP has an ID. This is different from the thread IDs used by the OS. We\n","can obtain the thread ID using the `omp_get_thread_num()` function."],"id":"c9513b73"},{"cell_type":"code","metadata":{"id":"be6fb543"},"source":["int ID = omp_get_thread_num();\n","printf(\"Hello World from %d!\\n\", ID);\n","printf(\"Goodbye World from %d!\\n\", ID);"],"id":"be6fb543","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"67f4a20f"},"source":["Let's compile and run the program again."],"id":"67f4a20f"},{"cell_type":"code","metadata":{"id":"67f66040"},"source":["!gcc -o hello -fopenmp hello1.c \n","./hello "],"id":"67f66040","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"921ba4aa"},"source":["We can see that the output from the different threads is interleaved indicating the concurrency (and\n","non-determinism) of execution. \n","\n","### <a name=\"timing\"></a>Performance Evaluation\n","We can measure the execution time of a parallel OpenMP program just like we would a sequential\n","program. Let's time the sequential version first. Instead of hard coding the number of threads, we can\n","pass the value to the program as a command-line argument."],"id":"921ba4aa"},{"cell_type":"code","metadata":{"attributes":{"classes":["C"],"id":""},"id":"22823729"},"source":["#include<stdio.h>\n","#include<stdlib.h>\n","#include<omp.h>\n","\n","int main(int argc, char* argv[]) {\n","\n","  int num_threads;\n","  if (argc <= 1)\n","    num_threads = 1;\n","  else\n","    num_threads = atoi(argv[1]);\n","\n","  omp_set_num_threads(num_threads);"],"id":"22823729","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"349c2af3"},"source":["Now we can run the sequential version and time it as follows"],"id":"349c2af3"},{"cell_type":"code","metadata":{"id":"93baaa31"},"source":["!time ./hello 1"],"id":"93baaa31","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6b5a3228"},"source":["`time` does not give us good enough resolution for this tiny program. We can use `perf` to get\n","*somewhat* better measurements."],"id":"6b5a3228"},{"cell_type":"code","metadata":{"id":"4e09b3b0"},"source":["!perf stat ./hello 1"],"id":"4e09b3b0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5d902be4"},"source":["Performance counter stats for './hello 1':"],"id":"5d902be4"},{"cell_type":"code","metadata":{"id":"17099a45"},"source":["2.240399      task-clock (msec)         #    0.864 CPUs utilized          \n","       0      context-switches          #    0.000 K/sec                  \n","       0      cpu-migrations            #    0.000 K/sec                  \n","     118      page-faults               #    0.053 M/sec                  \n","         3,788,857      cycles                    #    1.691 GHz                    \n","         2,289,566      stalled-cycles-frontend   #   60.43% frontend cycles idle   \n","         1,618,024      stalled-cycles-backend    #   42.70% backend cycles idle    \n","         3,607,090      instructions              #    0.95  insn per cycle         \n","                                        #    0.63  stalled cycles per insn\n"," 628,934      branches                  #  280.724 M/sec                  \n","  19,700      branch-misses             #    3.13% of all branches        "],"id":"17099a45","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fe8c5b19"},"source":["0.002592641 seconds time elapsed\n","\n","Now, let's run the code with 2 threads."],"id":"fe8c5b19"},{"cell_type":"code","metadata":{"id":"d3aa854a"},"source":["!perf stat ./hello 2"],"id":"d3aa854a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d75b1fef"},"source":["### <a name=\"thread_count\"></a>Thread Count and Scalability \n","\n","_How much performance improvement do we get by running this code in parallel?_\n","\n","This very simple code is not useful for doing any kind of performance evaluation. Let's look at a code\n","that is slightly more complex."],"id":"d75b1fef"},{"cell_type":"code","metadata":{"attributes":{"classes":["C"],"id":""},"id":"471a5530"},"source":["for p(j = 0; j < M; j++)\n","  for (i = 0; i < M; i++)\n","    b[i][j] = i + j;\n","\n","t0 = mysecond();\n","#pragma omp parallel for\n","  for (int k = 0; k < REPS; k++) {\n","    for (int j = 0; j < M; j++)\n","      for (int i = 0; i < M; i++)\n","        a[i][j] = b[i][j] * 17;\n","  }\n","\n","t0 = (mysecond() - t0) * 1.e3;\n","printf(\"parallel loop = %3.2f ms\\n\", t0);"],"id":"471a5530","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79e887e2"},"source":["The above program scales the values in an array by a constant factor. The loop is parallelized with the\n","`parallel for` directive. This directive is an extension of the `parallel` directive and is applied\n","exclusively to the *next* for loop. The `parallel for` directive will equally divide the iterations\n","of the loop and run them in parallel. The number of threads to be created is passed via a command-line\n","argument. There's a built-in timer to record the execution time of the parallel loop. \n","\n","Let's build and execute the sequential version of the code."],"id":"79e887e2"},{"cell_type":"code","metadata":{"id":"c7f47982"},"source":["!g++ -o scale scale.c -fopenmp\n","!./scale 1000 1"],"id":"c7f47982","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6b44b0f"},"source":["Let's run it with 2 threads."],"id":"d6b44b0f"},{"cell_type":"code","metadata":{"id":"5a7408f4"},"source":["!./scale 1000 2"],"id":"5a7408f4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ef2d79a"},"source":["Note, even with this very simple code we are not able to double the performance with 2 threads.  Now\n","let's run it with 12 threads which is what OpenMP picked for this system."],"id":"1ef2d79a"},{"cell_type":"code","metadata":{"id":"e90074f8"},"source":["!./scale 1000 12"],"id":"e90074f8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdb5f363"},"source":["_What if we kept on increasing the number of threads, do we expect more parallelism?_"],"id":"bdb5f363"},{"cell_type":"code","metadata":{"id":"2afc2d92"},"source":["./scale 1000 32\n","./scale 1000 64\n","./scale 1000 128"],"id":"2afc2d92","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0279d1f"},"source":["_Does this performance pattern reminds us of something?_\n","\n","This program becomes compute-bound when the number of threads is substantially higher than the available\n","processing cores. The ideal number of threads for a given program depends on many factors. Often some\n","fine-tuning is necessary. For instance, let's run the `scale` with 16 threads."],"id":"e0279d1f"},{"cell_type":"code","metadata":{"id":"04f58165"},"source":["./scale 1000 16"],"id":"04f58165","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"de369714"},"source":["This performance is worse than the performance achieved with 12 threads and 32 threads."],"id":"de369714"}]}